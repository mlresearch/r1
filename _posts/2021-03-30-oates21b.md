---
title: The Effects of Training Set Size on Decision Tree Complexity
abstract: 'This paper presents experiments with 19 datasets and 5 decision tree pruning
  algorithms that show that increasing training set size often results in a linear
  increase in tree size, even when that additional complexity results in no significant
  increase in classification accuracy. Said differently, removing randomly selected
  training instances often results in trees that are substantially smaller and just
  as accurate as those built on all available training instances. This implies that
  decreases in tree size obtained by more sophisticated data reduction techniques
  should be decomposed into two parts: that which is due to reduction of training
  set size, and the remainder, which is due to how the method selects instances to
  discard. We perform this decomposition for one recent data reduction technique,
  Johnâ€™s ROBUST-c4.5 (John 1995), and show that a large percentage of its effect on
  tree size is attributable to the fact that it simply reduces the size of the training
  set. We conclude that random data reduction is a baseline against which more sophisticated
  data reduction techniques should be compared.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: oates21b
month: 0
tex_title: The Effects of Training Set Size on Decision Tree Complexity
firstpage: 379
lastpage: 390
page: 379-390
order: 379
cycles: false
bibtex_author: Oates, Tim and Jensen, David
author:
- given: Tim
  family: Oates
- given: David
  family: Jensen
date: 2021-03-30
address:
container-title: Proceedings of the Sixth International Workshop on Artificial Intelligence
  and Statistics
volume: R1
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 3
  - 30
pdf: http://proceedings.mlr.press/r1/oates21b/oates21b.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
