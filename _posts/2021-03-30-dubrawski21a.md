---
title: Memory Based Stochastic Optimization for Validation and Tuning of Function
  Approximators
abstract: This paper focuses on the optimization of hyper-parameters for function
  approximators. We describe a kind of racing algorithm for continuous optimization
  problems that spends less time evaluating poor parameter settings and more time
  honing its estimates in the most promising regions of the parameter space. The algorithm
  is able to automatically optimize the parameters of a function approximator with
  less computation time. We demonstrate the algorithm on the problem of finding good
  parameters for a memory based learner and show the tradeoffs involved in choosing
  the right amount of computation to spend on each evaluation.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: dubrawski21a
month: 0
tex_title: Memory Based Stochastic Optimization for Validation and Tuning of Function
  Approximators
firstpage: 165
lastpage: 172
page: 165-172
order: 165
cycles: false
bibtex_author: Dubrawski, Artur and Schneider, Jeff
author:
- given: Artur
  family: Dubrawski
- given: Jeff
  family: Schneider
date: 2021-03-30
address:
container-title: Proceedings of the Sixth International Workshop on Artificial Intelligence
  and Statistics
volume: '1'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 3
  - 30
pdf: http://proceedings.mlr.press/v1/dubrawski21a/dubrawski21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
