---
title: Mixed Memory Markov Models
abstract: We consider how to parameterize Markov models with prohibitively large state
  spaces. This is done by representing the transition matrix as a convex combination-or
  mixtureof simpler dynamical models. The parameters in these models admit a simple
  probabilistic interpretation and can be fitted iteratively by an Expectation-Maximization
  (EM) procedure. We give examples where these models may be a faithful and/or useful
  representation of the underlying dynamics. We also derive a set of generalized Baum-Welch
  updates for hidden Markov models (HMMs) that make use of this parameterization.
  Because these models decompose the hidden state as the Cartesian product of two
  or more random variables, they are well suited to the modeling of coupled time series.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: saul21a
month: 0
tex_title: Mixed Memory {M}arkov Models
firstpage: 437
lastpage: 444
page: 437-444
order: 437
cycles: false
bibtex_author: Saul, Lawrence K. and Jordan, Michael I.
author:
- given: Lawrence K.
  family: Saul
- given: Michael I.
  family: Jordan
date: 1997-01-05
note: Reissued by PMLR on 30 March 2021.
address:
container-title: Proceedings of the Sixth International Workshop on Artificial Intelligence
  and Statistics
volume: R1
genre: inproceedings
issued:
  date-parts:
  - 1997
  - 1
  - 5
pdf: http://proceedings.mlr.press/r1/saul21a/saul21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
