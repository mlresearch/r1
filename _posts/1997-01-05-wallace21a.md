---
title: MML Mixture Modelling of Multi-state, Poisson, vonMises circular and Gaussian
  Distributions
abstract: Minimum Message Length (MML) is an invariant Bayesian point estimation technique
  which is also consistent and efficient. We provide a brief overview of MML inductive
  inference (Wallace and Boulton (1968) , Wallace and Freeman (1987)), and how it
  has both an information-theoretic and a Bayesian interpretation. We then outline
  how MML is used for statistical parameter estimation, and how the MML mixture modelling
  program, Snob (Wallace and Boulton (1968), Wallace (1986), Wallace and Dowe (1994))
  uses the message lengths from various parameter estimates to enable it to combine
  parameter estimation with selection of the num- ber of components. The message length
  is (to within a constant) the logarithm of the posterior probability of the theory.
  So, the MML theory can also be re- garded as the theory with the highest posterior
  probability. Snob currently assumes that variables are uncorrelated, and permits
  multi-variate data from Gaussian, discrete multi-state, Poisson and von Mises circular
  distributions.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wallace21a
month: 0
tex_title: MML Mixture Modelling of Multi-state, Poisson, vonMises circular and Gaussian
  Distributions
firstpage: 529
lastpage: 536
page: 529-536
order: 529
cycles: false
bibtex_author: Wallace, Chris S. and Dowe, David L.
author:
- given: Chris S.
  family: Wallace
- given: David L.
  family: Dowe
date: 1997-01-05
notes: Reissued by PMLR on 30 March 2021.
address:
container-title: Proceedings of the Sixth International Workshop on Artificial Intelligence
  and Statistics
volume: R1
genre: inproceedings
issued:
  date-parts:
  - 1997
  - 1
  - 5
pdf: http://proceedings.mlr.press/r1/wallace21a/wallace21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
